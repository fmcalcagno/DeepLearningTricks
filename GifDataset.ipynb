{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch Giff Dataset Class for loading gifs using Pytorch DataLoader.\n",
    "This Dataset resizes all frames to a specific size.\n",
    "\n",
    "Facundo Calcagno\n",
    "\"\"\"\n",
    "from __future__ import print_function, division\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image,ImageFilter , ImageOps, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gifDataSet(Dataset):\n",
    "    \"\"\"Dataset Class for loading Gif into a 3D Tensor\"\"\"\n",
    "    def __init__(self,gifList,rootDir, channels, timeDepth, xSize, ySize, startFrame,endFrame,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        clipsList (string): Path to the clipsList file with labels.\n",
    "        rootDir (string): Directory with all the gifs.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "        channels: Number of channels of frames\n",
    "        timeDepth: Number of frames to be loaded in a sample\n",
    "        xSize, ySize: Dimensions of the frames\n",
    "        mean: Mean valuse of the training set videos over each channel\n",
    "        \"\"\"\n",
    "        self.gifList = gifList\n",
    "        self.rootDir = rootDir\n",
    "        self.channels = channels\n",
    "        self.timeDepth = timeDepth\n",
    "        self.xSize = xSize\n",
    "        self.ySize = ySize\n",
    "        #self.mean = mean\n",
    "        self.transform = transform\n",
    "        self.startFrame=startFrame\n",
    "        self.endFrame=endFrame\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gifList)\n",
    "    \n",
    "    def crop6(self,im):\n",
    "        number_of_cols=3\n",
    "        W=im.width\n",
    "        H=im.height\n",
    "        w=(W-16)/3\n",
    "        h=(H-24)/2\n",
    "        images=[]\n",
    "        w1=0\n",
    "        w2=w\n",
    "        for i in range(number_of_cols):\n",
    "            im1=im.crop((w1, 0, w2, h))\n",
    "            images.append(im1)\n",
    "            im1=im.crop((w1, h+8, w2, 2*h+8))\n",
    "            images.append(im1)\n",
    "            w1=w2+8\n",
    "            w2=w2+w+8\n",
    "        return images\n",
    "    \n",
    "    def readGif(self, gifFile):\n",
    "        # Open the gif file, crop it and return the frames in a tensor\n",
    "        #cap= cv2.VideoCapture(gifFile)\n",
    "        image_gif=Image.open(self.rootDir+ gifFile, mode='r')\n",
    "        frames = torch.FloatTensor(self.timeDepth,  self.xSize, self.ySize,self.channels)\n",
    "        nframes = 0\n",
    "        nframesin=0\n",
    "        while image_gif:\n",
    "            if self.startFrame<nframes<self.endFrame:\n",
    "                six_images=self.crop6(image_gif)\n",
    "                pil_image = six_images[4].convert(\"RGB\")\n",
    "                imResize=pil_image.resize((self.xSize, self.ySize),Image.ANTIALIAS)\n",
    "                frame = torch.from_numpy(np.array(imResize))\n",
    "                print(frame.size())\n",
    "                #frame = frame.permute(2, 0, 1)\n",
    "\n",
    "                frames[nframesin, :, :, :] = frame\n",
    "                nframesin+=1\n",
    "            nframes += 1\n",
    "            try:\n",
    "                image_gif.seek( nframes )\n",
    "            except EOFError:\n",
    "                break;\n",
    "            \n",
    "        image_gif.close()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir=\"gifs/\"\n",
    "import glob\n",
    "mypath=rootDir+'Mediophyceae_Hemiaulus'\n",
    "a=glob.glob(rootDir+'*.gif')\n",
    "#with open('gifs.pickle', 'wb') as fp:\n",
    "#    pickle.dump(a, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gifListFile='gifs.pickle'\n",
    "rootDir=\"gifs/\"\n",
    "channels=3\n",
    "timeDepth=3\n",
    "xSize=100\n",
    "ySize=100\n",
    "startFrame=1\n",
    "endFrame=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gifDataset= gifDataSet(a,rootDir,channels,timeDepth,xSize,ySize,startFrame,endFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100, 3])\n",
      "torch.Size([100, 100, 3])\n",
      "torch.Size([100, 100, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor=gifDataset.readGif(\"10753468_S127--D0--R27--G100010824--A131203--L02236--animation.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 184.,  184.,  184.],\n",
       "         [ 182.,  182.,  182.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         ...,\n",
       "         [ 182.,  182.,  182.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 187.,  187.,  187.]],\n",
       "\n",
       "        [[ 185.,  185.,  185.],\n",
       "         [ 186.,  186.,  186.],\n",
       "         [ 184.,  184.,  184.],\n",
       "         ...,\n",
       "         [ 182.,  182.,  182.],\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 181.,  181.,  181.]],\n",
       "\n",
       "        [[ 184.,  184.,  184.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 184.,  184.,  184.],\n",
       "         ...,\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 184.,  184.,  184.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 182.,  182.,  182.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 181.,  181.,  181.],\n",
       "         ...,\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 184.,  184.,  184.]],\n",
       "\n",
       "        [[ 184.,  184.,  184.],\n",
       "         [ 184.,  184.,  184.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         ...,\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 185.,  185.,  185.]],\n",
       "\n",
       "        [[ 184.,  184.,  184.],\n",
       "         [ 183.,  183.,  183.],\n",
       "         [ 185.,  185.,  185.],\n",
       "         ...,\n",
       "         [ 185.,  185.,  185.],\n",
       "         [ 186.,  186.,  186.],\n",
       "         [ 184.,  184.,  184.]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
